{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Read Datasets & PII Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviornment for developing a quick method to scan through datasets and see what PPID and PID are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pii_violation(csv_filePath: str, pii_roots = ['Name', 'Date', 'Time', 'Address', 'Residence', 'County', 'State', 'District', 'Street', 'Code', 'Number', 'Age', 'Ethnicity', 'Gender', 'Occupation', 'Status', 'DOB', 'Year', 'Month', 'Day']):\n",
    "    # read in the CSV\n",
    "    data = pd.read_csv(csv_filePath)  \n",
    "\n",
    "    # create a lowercase version of whatever was passed in or the default list\n",
    "    pii_roots = [x.lower() for x in pii_roots] \n",
    "\n",
    "    # container for the hit columns\n",
    "    suspected_pii = []\n",
    "\n",
    "    # check each of the columns against our pii_roots (the default or a custom one)\n",
    "    for col in data.columns:\n",
    "        # make sure everything is the same regardless of casing\n",
    "        lowercol = col.lower()\n",
    "\n",
    "        # check the lower case versions, but append the normal column\n",
    "        if len(set(lowercol.split(' ')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have a space as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "\n",
    "        elif len(set(lowercol.split('_')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have '_' as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "        elif len(set(lowercol.split('-')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have '-' as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "\n",
    "    # hits in comparison to the columns\n",
    "    hitRate = round(len(suspected_pii) / len(data.columns), 2)\n",
    "\n",
    "    return suspected_pii, hitRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class\n",
    "\n",
    "Upgrades the function into something more robuts and useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIIScan():\n",
    "    def __init__(self, filePath: str, pii = ['Name', 'Date', 'SSN', 'Time', 'Address', 'Street', 'Residence', 'Country', 'County', 'State', 'District', 'Code', 'Number', 'Age', 'Ethnicity', 'Gender', 'Occupation', 'Status', 'DOB', 'Year', 'Month', 'Day']):\n",
    "        '''\n",
    "        Reads a dataset (csv or xlsx supported) given a filepath, saves it and the metadata, and checks for partial or complete PII matches.\n",
    "\n",
    "        Inputs:\n",
    "            - (str) filePath: the relative or full path to the CSV or XLSX file\n",
    "            - (list-like) pii: optional list of custom key words that we want to locate in the dataset features that may be PII. Default provided.\n",
    "\n",
    "        Returns:\n",
    "            - None. All information is saved to the object.\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (str) filePath: The relative or full path to the data that was provided\n",
    "            - (str) fileName: The name of the file minus the extension\n",
    "            - (str) fileExtension: The extension name like 'csv' or 'xlsx'\n",
    "            - (list: str) features: All column (feature) names\n",
    "            - (list: str) roots: All key words that we assume are associated with PII (default provided)\n",
    "\n",
    "        Attributes Defined Elsewhere:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "            - (float) hitRate: Proportion of possibly PII columns in the dataset. Rounded to 2 decimal places)\n",
    "            - (pd.Series) nan: Feature name and proportion of it's entries that are NAN\n",
    "        '''\n",
    "        \n",
    "        # save the file path, name, and extension (mainly for report purposes)\n",
    "        self.filePath = filePath\n",
    "        self.fileName = Path(filePath).stem\n",
    "        self.fileExtension = filePath.split('.')[-1]\n",
    "\n",
    "        # get and save the data\n",
    "        self._readFile()\n",
    "        self.features = list(self.df.columns)\n",
    "        self.featsMissing = self.getNan()\n",
    "\n",
    "        # save the lowercase version of roots passed in or used from default\n",
    "        self.roots = [root.lower() for root in pii]\n",
    "\n",
    "        # check for PII hits\n",
    "        print('Checking for PII Violations...')\n",
    "        self._piiViolation()\n",
    "\n",
    "        print('Reporting...\\n')\n",
    "\n",
    "        print(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f' File: {self.fileName} \\n File Type: {self.fileExtension} \\n\\n Features: {len(self.features)} \\n Features with Missingness: {self.featsMissing.shape[0]} \\n Records: {len(self.df)} \\n\\n Raw Hit Rate: {self.hitRate} \\n\\n Possible PII Matches: {len(self.matches)} \\n Possible Matches: {self.matches} \\n\\n Keyword Hit Rate: {self.kwHR} \\n Keyword Matches: {self.kwMatches}'\n",
    "\n",
    "    def _readFile(self):\n",
    "        '''\n",
    "        Reads the datafile into a pandas DataFrame. Used in the initialization of the object.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (pd.Series) nan: Feature name and proportion of it's entries that are NAN\n",
    "        '''\n",
    "        \n",
    "        if self.fileExtension == 'csv':\n",
    "            print('Reading CSV...')\n",
    "        \n",
    "            self.df = pd.read_csv(self.filePath)\n",
    "        \n",
    "        elif self.fileExtension == 'xlsx':\n",
    "            print('Reading XLSX...')\n",
    "\n",
    "            self.df = pd.read_excel(self.filePath)\n",
    "        \n",
    "        else:\n",
    "            print(f'Extension not recognized: {self.fileExtension}')\n",
    "\n",
    "        # Feature NAN analysis (what proportion of the feature is NAN?)\n",
    "        print('Running feature NAN Analysis...')\n",
    "        self.nan = self.df.isna().mean()\n",
    "        \n",
    "    def _piiViolation(self):\n",
    "        '''\n",
    "        Runs the PII-matching method. Given the pandas representation of the data, look at the columns (features) and record the ones that contain the PII keywords we defined.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "            - (float) hitRate: Proportion of possibly PII columns in the dataset. Rounded to 2 decimal places\n",
    "            - (float) kwHR: The hit rate relative to the keywords, rounded to 2 decimal places\n",
    "        '''\n",
    "                \n",
    "        # container for the hit columns and the keyword we matched\n",
    "        suspected_pii = []\n",
    "        kwType = []\n",
    "\n",
    "        # check each of the columns against our pii_roots (the default or a custom one)\n",
    "        for col in self.features:\n",
    "            # make sure everything is the same regardless of casing\n",
    "            lowercol = col.lower()\n",
    "\n",
    "            # try to break the feature name in different ways\n",
    "            spaceSplit = set(lowercol.split(' ')).intersection(self.roots)\n",
    "            underSplit = set(lowercol.split('_')).intersection(self.roots)\n",
    "            dashSplit = set(lowercol.split('-')).intersection(self.roots)\n",
    "            camelSplit = set(self._detectCC(col)).intersection(self.roots)\n",
    "\n",
    "            # check the lower case versions, but append the normal column\n",
    "            if len(spaceSplit) > 0:\n",
    "                # the roots matched, so the column(s) have a space as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(spaceSplit))\n",
    "\n",
    "            elif len(underSplit) > 0:\n",
    "                # the roots matched, so the column(s) have '_' as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(underSplit))\n",
    "\n",
    "            elif len(dashSplit) > 0:\n",
    "                # the roots matched, so the column(s) have '-' as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(dashSplit))\n",
    "\n",
    "            elif len(camelSplit) > 0:\n",
    "                # we may have a camel case situation\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(camelSplit))\n",
    "\n",
    "        # if we found something \n",
    "        if len(kwType) > 0:\n",
    "            kwType = list(np.unique(kwType))\n",
    "\n",
    "        # get rid of any duplicates we've amassed and save it\n",
    "        self.matches = list(np.unique(suspected_pii))\n",
    "        self.kwMatches = kwType\n",
    "\n",
    "        self.hitRate = round(len(suspected_pii) / len(self.features), 2)\n",
    "        # self.adjHR = round(len(suspected_pii) / len(set(suspected_pii) - set(self.featsMissing)), 2)\n",
    "        self.kwHR = round(len(kwType) / len(self.roots), 2)\n",
    "\n",
    "    def _detectCC(self, str):\n",
    "        '''\n",
    "        Adapted from GeeksForGeeks article Python | Split CamelCase string to individual strings.\n",
    "\n",
    "        Inputs:\n",
    "            - (str) str: The column name we're looking at for possible PII to see if it's CamelCase\n",
    "\n",
    "        Returns:\n",
    "            - (list: str): List of the words we found based on splitting up via CamelCase\n",
    "        '''\n",
    "        # set the word list with the very first letter in the string\n",
    "        words = [[str[0]]]\n",
    "    \n",
    "        # looks at the rest of the characters\n",
    "        for c in str[1:]:\n",
    "            if words[-1][-1].islower() and c.isupper():\n",
    "                words.append(list(c.lower()))\n",
    "\n",
    "            else:\n",
    "                words[-1].append(c.lower())\n",
    "    \n",
    "        # re-builds the words into strings\n",
    "        return [''.join(word) for word in words]\n",
    "\n",
    "    def getData(self):\n",
    "        '''\n",
    "        Getter for the dataframe and the possible PII matches. \n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "        '''\n",
    "        return self.df, self.matches\n",
    "    \n",
    "    def getMatchSet(self):\n",
    "        '''\n",
    "        Getter for the auto-subset df of the possible matches.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.DataFrame): Subset data stored in DataFrame format\n",
    "        '''\n",
    "        return self.df[self.matches]\n",
    "    \n",
    "    def getMatchSet_Latex(self, rows = 5):\n",
    "        '''\n",
    "        Getter for the auto-subset df of the possible matches, but in latex format.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (str): Latex tabular representation of the head of the data\n",
    "        '''\n",
    "        return print(self.df[self.matches].head(rows).to_latex(index = False))\n",
    "    \n",
    "    def getNan(self):\n",
    "        '''\n",
    "        Returns the columns that have some number of NANs in them.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.Series): Column (feature) names that have a proportion of NANs that greater than 0\n",
    "        '''\n",
    "\n",
    "        return self.nan[self.nan > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/s4dmzgv113lb3_sdwwd28tv40000gn/T/ipykernel_91324/1425919820.py:69: DtypeWarning: Columns (30,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(self.filePath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature NAN Analysis...\n",
      "Checking for PII Violations...\n",
      "Reporting...\n",
      "\n",
      " File: DallasArrests \n",
      " File Type: csv \n",
      "\n",
      " Features: 65 \n",
      " Features with Missingness: 58 \n",
      " Records: 105161 \n",
      "\n",
      " Raw Hit Rate: 0.26 \n",
      "\n",
      " Possible PII Matches: 17 \n",
      " Possible Matches: ['Age', 'AgeAtArrestTime', 'AliasName', 'ArArrestDate', 'ArArrestTime', 'ArBkDate', 'ArLAddress', 'ArLCounty', 'ArLDistrict', 'ArState', 'ArrestNumber', 'ArresteeName', 'CFS_Number', 'JobSchStatus', 'NickName', 'Occupation', 'UpZDate'] \n",
      "\n",
      " Keyword Hit Rate: 0.5 \n",
      " Keyword Matches: ['address', 'age', 'county', 'date', 'district', 'name', 'number', 'occupation', 'state', 'status', 'time']\n"
     ]
    }
   ],
   "source": [
    "dallas = PIIScan('../data/DallasArrests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dallas.getNan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dallas.getMatchSet_Latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = PIIScan('../data/chicagoParole.csv')\n",
    "# one, two = this.getData()\n",
    "# chicagoZip = [60601, 60602, 60603, 60604, 60605, 60606, 60607, 60608, 60609, 60610, 60611, 60612, 60613, 60614, 60615, 60616, 60617, 60618, 60619, 60620, 60621, 60622, 60623, 60624, 60625, 60626, 60628, 60629, 60630, 60631, 60632, 60633, 60634, 60636, 60637, 60638, 60639, 60640, 60641, 60642, 60643, 60644, 60645, 60646, 60647, 60649, 60651, 60652, 60653, 60654, 60655, 60656, 60657, 60659, 60660, 60661, 60664, 60666, 60668, 60669, 60670, 60673, 60674, 60675, 60677, 60678, 60680, 60681, 60682, 60684, 60685, 60686, 60687, 60688, 60689, 60690, 60691, 60693, 60694, 60695, 60696, 60697, 60699, 60701, 60706, 60707, 60803, 60804, 60805, 60827]\n",
    "# one[[True if np.isin(i, chicagoZip) else False for i in one['Residence Zip Code']]].to_csv('../data/chicagoParole2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
