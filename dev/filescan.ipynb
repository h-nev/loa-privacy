{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Read Datasets & PII Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviornment for developing a quick method to scan through datasets and see what PPID and PID are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pii_violation(csv_filePath: str, pii_roots = ['Name', 'Date', 'Time', 'Address', 'Residence', 'County', 'State', 'District', 'Street', 'Code', 'Number', 'Age', 'Ethnicity', 'Gender', 'Occupation', 'Status', 'DOB', 'Year', 'Month', 'Day']):\n",
    "    # read in the CSV\n",
    "    data = pd.read_csv(csv_filePath)  \n",
    "\n",
    "    # create a lowercase version of whatever was passed in or the default list\n",
    "    pii_roots = [x.lower() for x in pii_roots] \n",
    "\n",
    "    # container for the hit columns\n",
    "    suspected_pii = []\n",
    "\n",
    "    # check each of the columns against our pii_roots (the default or a custom one)\n",
    "    for col in data.columns:\n",
    "        # make sure everything is the same regardless of casing\n",
    "        lowercol = col.lower()\n",
    "\n",
    "        # check the lower case versions, but append the normal column\n",
    "        if len(set(lowercol.split(' ')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have a space as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "\n",
    "        elif len(set(lowercol.split('_')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have '_' as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "        elif len(set(lowercol.split('-')).intersection(pii_roots)) > 0:\n",
    "            # the roots matched, so the column(s) have '-' as a delimiter\n",
    "            suspected_pii.append(col)\n",
    "\n",
    "    # hits in comparison to the columns\n",
    "    hitRate = round(len(suspected_pii) / len(data.columns), 2)\n",
    "\n",
    "    return suspected_pii, hitRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class\n",
    "\n",
    "Upgrades the function into something more robuts and useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIIScan():\n",
    "    def __init__(self, filePath: str, pii = ['Name', 'Date', 'SSN', 'Time', 'Address', 'Street', 'Residence', 'Country', 'County', 'State', 'District', 'Code', 'Number', 'Age', 'Ethnicity', 'Gender', 'Sex', 'Occupation', 'Status', 'DOB', 'Year', 'Month', 'Day']):\n",
    "        '''\n",
    "        Reads a dataset (csv or xlsx supported) given a filepath, saves it and the metadata, and checks for partial or complete PII matches.\n",
    "\n",
    "        Inputs:\n",
    "            - (str) filePath: the relative or full path to the CSV or XLSX file\n",
    "            - (list-like) pii: optional list of custom key words that we want to locate in the dataset features that may be PII. Default provided.\n",
    "\n",
    "        Returns:\n",
    "            - None. All information is saved to the object.\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (str) filePath: The relative or full path to the data that was provided\n",
    "            - (str) fileName: The name of the file minus the extension\n",
    "            - (str) fileExtension: The extension name like 'csv' or 'xlsx'\n",
    "            - (list: str) features: All column (feature) names\n",
    "            - (list: str) roots: All key words that we assume are associated with PII (default provided)\n",
    "\n",
    "        Attributes Defined Elsewhere:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "            - (float) hitRate: Proportion of possibly PII columns in the dataset. Rounded to 2 decimal places)\n",
    "            - (pd.Series) nan: Feature name and proportion of it's entries that are NAN\n",
    "        '''\n",
    "        \n",
    "        # save the file path, name, and extension (mainly for report purposes)\n",
    "        self.filePath = filePath\n",
    "        self.fileName = Path(filePath).stem\n",
    "        self.fileExtension = filePath.split('.')[-1]\n",
    "\n",
    "        # get and save the data\n",
    "        self._readFile()\n",
    "        self.features = list(self.df.columns)\n",
    "        self.featsMissing = self.getNan()\n",
    "\n",
    "        # save the lowercase version of roots passed in or used from default\n",
    "        self.roots = [root.lower() for root in pii]\n",
    "\n",
    "        # check for PII hits\n",
    "        print('Checking for PII Violations...')\n",
    "        self._piiViolation()\n",
    "\n",
    "        print('Reporting...\\n')\n",
    "\n",
    "        print(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'File: {self.fileName} \\nFile Type: {self.fileExtension} \\n\\nFeatures: {len(self.features)} \\nRecords: {len(self.df)} \\n\\nRaw Hit Rate: {self.hitRate} \\n\\nPossible PII Matches: {len(self.matches)} \\nPossible Matches: {self.matches} \\n\\nKeyword Hit Rate: {self.kwHR} \\nKeyword Matches: {self.kwMatches} \\n\\nFeatures with Missingness: {self.featsMissing.shape[0]} \\nMissing Features: \\n{self.featsMissing.to_string(dtype=False)}'\n",
    "\n",
    "    def _readFile(self):\n",
    "        '''\n",
    "        Reads the datafile into a pandas DataFrame. Used in the initialization of the object.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (pd.Series) nan: Feature name and proportion of it's entries that are NAN\n",
    "        '''\n",
    "        \n",
    "        if self.fileExtension == 'csv':\n",
    "            print('Reading CSV...')\n",
    "        \n",
    "            self.df = pd.read_csv(self.filePath)\n",
    "        \n",
    "        elif self.fileExtension == 'xlsx':\n",
    "            print('Reading XLSX...')\n",
    "\n",
    "            self.df = pd.read_excel(self.filePath)\n",
    "        \n",
    "        else:\n",
    "            print(f'Extension not recognized: {self.fileExtension}')\n",
    "\n",
    "        # Feature NAN analysis (what proportion of the feature is NAN?)\n",
    "        print('Running feature NAN Analysis...')\n",
    "        self.nan = self.df.isna().mean()\n",
    "        \n",
    "    def _piiViolation(self):\n",
    "        '''\n",
    "        Runs the PII-matching method. Given the pandas representation of the data, look at the columns (features) and record the ones that contain the PII keywords we defined.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "\n",
    "        Attributes Defined Here:\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "            - (float) hitRate: Proportion of possibly PII columns in the dataset. Rounded to 2 decimal places\n",
    "            - (float) kwHR: The hit rate relative to the keywords, rounded to 2 decimal places\n",
    "        '''\n",
    "                \n",
    "        # container for the hit columns and the keyword we matched\n",
    "        suspected_pii = []\n",
    "        kwType = []\n",
    "\n",
    "        # check each of the columns against our pii_roots (the default or a custom one)\n",
    "        for col in self.features:\n",
    "            # make sure everything is the same regardless of casing\n",
    "            lowercol = col.lower()\n",
    "\n",
    "            # try to break the feature name in different ways\n",
    "            spaceSplit = set(lowercol.split(' ')).intersection(self.roots)\n",
    "            underSplit = set(lowercol.split('_')).intersection(self.roots)\n",
    "            dashSplit = set(lowercol.split('-')).intersection(self.roots)\n",
    "            camelSplit = set(self._detectCC(col)).intersection(self.roots)\n",
    "\n",
    "            # check the lower case versions, but append the normal column\n",
    "            if len(spaceSplit) > 0:\n",
    "                # the roots matched, so the column(s) have a space as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(spaceSplit))\n",
    "\n",
    "            elif len(underSplit) > 0:\n",
    "                # the roots matched, so the column(s) have '_' as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(underSplit))\n",
    "\n",
    "            elif len(dashSplit) > 0:\n",
    "                # the roots matched, so the column(s) have '-' as a delimiter\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(dashSplit))\n",
    "\n",
    "            elif len(camelSplit) > 0:\n",
    "                # we may have a camel case situation\n",
    "                suspected_pii.append(col)\n",
    "                kwType.extend(list(camelSplit))\n",
    "\n",
    "        # if we found something \n",
    "        if len(kwType) > 0:\n",
    "            kwType = list(np.unique(kwType))\n",
    "\n",
    "        # get rid of any duplicates we've amassed and save it\n",
    "        self.matches = list(np.unique(suspected_pii))\n",
    "        self.kwMatches = kwType\n",
    "\n",
    "        self.hitRate = round(len(suspected_pii) / len(self.features), 2)\n",
    "        # self.adjHR = round(len(suspected_pii) / len(set(suspected_pii) - set(self.featsMissing)), 2)\n",
    "        self.kwHR = round(len(kwType) / len(self.roots), 2)\n",
    "\n",
    "    def _detectCC(self, str):\n",
    "        '''\n",
    "        Adapted from GeeksForGeeks article Python | Split CamelCase string to individual strings.\n",
    "\n",
    "        Inputs:\n",
    "            - (str) str: The column name we're looking at for possible PII to see if it's CamelCase\n",
    "\n",
    "        Returns:\n",
    "            - (list: str): List of the words we found based on splitting up via CamelCase\n",
    "        '''\n",
    "        # set the word list with the very first letter in the string\n",
    "        words = [[str[0]]]\n",
    "    \n",
    "        # looks at the rest of the characters\n",
    "        for c in str[1:]:\n",
    "            if words[-1][-1].islower() and c.isupper():\n",
    "                words.append(list(c.lower()))\n",
    "\n",
    "            else:\n",
    "                words[-1].append(c.lower())\n",
    "    \n",
    "        # re-builds the words into strings\n",
    "        return [''.join(word) for word in words]\n",
    "\n",
    "    def getData(self):\n",
    "        '''\n",
    "        Getter for the dataframe and the possible PII matches. \n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.DataFrame) df: Data stored in DataFrame format\n",
    "            - (list: str) matches: List of unique column (feature) names that were found to be partial or complete matches to something in the PII list\n",
    "        '''\n",
    "        return self.df, self.matches\n",
    "    \n",
    "    def getMatchSet(self):\n",
    "        '''\n",
    "        Getter for the auto-subset df of the possible matches.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.DataFrame): Subset data stored in DataFrame format\n",
    "        '''\n",
    "        return self.df[self.matches]\n",
    "    \n",
    "    def getMatchSet_Latex(self, rows = 5):\n",
    "        '''\n",
    "        Getter for the auto-subset df of the possible matches, but in latex format.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (str): Latex tabular representation of the head of the data\n",
    "        '''\n",
    "        return print(self.df[self.matches].head(rows).to_latex(index = False))\n",
    "    \n",
    "    def getNan(self):\n",
    "        '''\n",
    "        Returns the columns that have some number of NANs in them.\n",
    "\n",
    "        Inputs:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - (pd.Series): Column (feature) names that have a proportion of NANs that greater than 0\n",
    "        '''\n",
    "\n",
    "        return self.nan[self.nan > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Running feature NAN Analysis...\n",
      "Checking for PII Violations...\n",
      "Reporting...\n",
      "\n",
      "File: DallasArrests \n",
      "File Type: csv \n",
      "\n",
      "Features: 65 \n",
      "Records: 105161 \n",
      "\n",
      "Raw Hit Rate: 0.28 \n",
      "\n",
      "Possible PII Matches: 18 \n",
      "Possible Matches: ['Age', 'AgeAtArrestTime', 'AliasName', 'ArArrestDate', 'ArArrestTime', 'ArBkDate', 'ArLAddress', 'ArLCounty', 'ArLDistrict', 'ArState', 'ArrestNumber', 'ArresteeName', 'CFS_Number', 'JobSchStatus', 'NickName', 'Occupation', 'Sex', 'UpZDate'] \n",
      "\n",
      "Keyword Hit Rate: 0.52 \n",
      "Keyword Matches: ['address', 'age', 'county', 'date', 'district', 'name', 'number', 'occupation', 'sex', 'state', 'status', 'time'] \n",
      "\n",
      "Features with Missingness: 58 \n",
      "Missing Features: \n",
      "ArBkDate           0.002301\n",
      "ArLAddress         0.000190\n",
      "ArLApt             0.918030\n",
      "ArLZip             0.000352\n",
      "ArLCity            0.000770\n",
      "ArState            0.004051\n",
      "ArLCounty          0.018733\n",
      "ArLRA              0.162741\n",
      "ArLBeat            0.026217\n",
      "ArLDistrict        0.066612\n",
      "ArLSector          0.421820\n",
      "ArPremises         0.254610\n",
      "CFS_Number         0.005144\n",
      "ArOfcr1            0.000342\n",
      "ArOfcr2            0.263501\n",
      "Transport1         0.000609\n",
      "Transport2         0.410485\n",
      "Transport3         0.999971\n",
      "Search1            0.999990\n",
      "ArAction           0.000048\n",
      "ArWeapon           0.388300\n",
      "ArOSR              1.000000\n",
      "ArResisted         1.000000\n",
      "ArCurrLoc          0.026654\n",
      "ArCond             0.996596\n",
      "ArMedFlag          1.000000\n",
      "ArMedLoc           0.968486\n",
      "ArOpComm           1.000000\n",
      "ArresteeName       0.000105\n",
      "NickName           0.968819\n",
      "AliasName          0.993838\n",
      "BirthPlace         0.542482\n",
      "Age                0.001569\n",
      "AgeAtArrestTime    0.057398\n",
      "HAddress           0.020920\n",
      "HApt               0.755679\n",
      "HZIP               0.049400\n",
      "HCity              0.022594\n",
      "HState             0.045074\n",
      "HRA                0.483839\n",
      "HBeat              0.432508\n",
      "Height             0.001560\n",
      "Weight             0.001712\n",
      "Hair               0.035022\n",
      "Eyes               0.002910\n",
      "Race               0.000171\n",
      "Ethnic             0.000523\n",
      "Sex                0.000979\n",
      "Tatoo              0.956334\n",
      "TatooComment       0.977168\n",
      "Occupation         0.950733\n",
      "JobSchStatus       1.000000\n",
      "Employer           0.924449\n",
      "Drug               0.995512\n",
      "DrugRelated        0.170928\n",
      "DrugType           0.877863\n",
      "ClothingWorn       0.016090\n",
      "Expunged           1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/s4dmzgv113lb3_sdwwd28tv40000gn/T/ipykernel_75182/709416909.py:69: DtypeWarning: Columns (30,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(self.filePath)\n"
     ]
    }
   ],
   "source": [
    "dallas = PIIScan('../data/DallasArrests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Running feature NAN Analysis...\n",
      "Checking for PII Violations...\n",
      "Reporting...\n",
      "\n",
      "File: DallasArrests \n",
      "File Type: csv \n",
      "\n",
      "Features: 65 \n",
      "Records: 105161 \n",
      "\n",
      "Raw Hit Rate: 0.35 \n",
      "\n",
      "Possible PII Matches: 23 \n",
      "Possible Matches: ['Age', 'AgeAtArrestTime', 'AliasName', 'ArArrestDate', 'ArArrestTime', 'ArBkDate', 'ArCurrLoc', 'ArLAddress', 'ArLApt', 'ArLCounty', 'ArLDistrict', 'ArMedLoc', 'ArState', 'ArrestNumber', 'ArresteeName', 'CFS_Number', 'Employer', 'Height', 'IncidentNum', 'JobSchStatus', 'NickName', 'Occupation', 'UpZDate'] \n",
      "\n",
      "Keyword Hit Rate: 0.48 \n",
      "Keyword Matches: ['address', 'age', 'apt', 'county', 'date', 'district', 'employer', 'height', 'loc', 'name', 'num', 'number', 'occupation', 'state', 'status', 'time'] \n",
      "\n",
      "Features with Missingness: 58 \n",
      "Missing Features: \n",
      "ArBkDate           0.002301\n",
      "ArLAddress         0.000190\n",
      "ArLApt             0.918030\n",
      "ArLZip             0.000352\n",
      "ArLCity            0.000770\n",
      "ArState            0.004051\n",
      "ArLCounty          0.018733\n",
      "ArLRA              0.162741\n",
      "ArLBeat            0.026217\n",
      "ArLDistrict        0.066612\n",
      "ArLSector          0.421820\n",
      "ArPremises         0.254610\n",
      "CFS_Number         0.005144\n",
      "ArOfcr1            0.000342\n",
      "ArOfcr2            0.263501\n",
      "Transport1         0.000609\n",
      "Transport2         0.410485\n",
      "Transport3         0.999971\n",
      "Search1            0.999990\n",
      "ArAction           0.000048\n",
      "ArWeapon           0.388300\n",
      "ArOSR              1.000000\n",
      "ArResisted         1.000000\n",
      "ArCurrLoc          0.026654\n",
      "ArCond             0.996596\n",
      "ArMedFlag          1.000000\n",
      "ArMedLoc           0.968486\n",
      "ArOpComm           1.000000\n",
      "ArresteeName       0.000105\n",
      "NickName           0.968819\n",
      "AliasName          0.993838\n",
      "BirthPlace         0.542482\n",
      "Age                0.001569\n",
      "AgeAtArrestTime    0.057398\n",
      "HAddress           0.020920\n",
      "HApt               0.755679\n",
      "HZIP               0.049400\n",
      "HCity              0.022594\n",
      "HState             0.045074\n",
      "HRA                0.483839\n",
      "HBeat              0.432508\n",
      "Height             0.001560\n",
      "Weight             0.001712\n",
      "Hair               0.035022\n",
      "Eyes               0.002910\n",
      "Race               0.000171\n",
      "Ethnic             0.000523\n",
      "Sex                0.000979\n",
      "Tatoo              0.956334\n",
      "TatooComment       0.977168\n",
      "Occupation         0.950733\n",
      "JobSchStatus       1.000000\n",
      "Employer           0.924449\n",
      "Drug               0.995512\n",
      "DrugRelated        0.170928\n",
      "DrugType           0.877863\n",
      "ClothingWorn       0.016090\n",
      "Expunged           1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/s4dmzgv113lb3_sdwwd28tv40000gn/T/ipykernel_75182/709416909.py:69: DtypeWarning: Columns (30,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(self.filePath)\n"
     ]
    }
   ],
   "source": [
    "custom = ['Name', 'Date', 'SSN', 'Time', 'Address', 'Street', 'Residence', 'Country', 'County', 'State', 'District', 'Code', 'Number', 'Age', 'Ethnicity', 'Gender', 'Occupation', 'Status', 'DOB', 'Year', 'Month', 'Day', 'Ethnicity', 'No', 'Num', 'Loc', 'Location', 'District', 'Height', 'Employer', 'Job', 'Tattoo', 'Apt']\n",
    "dallas = PIIScan('../data/DallasArrests.csv', custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dallas.getNan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dallas.getMatchSet_Latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = PIIScan('../data/chicagoParole.csv')\n",
    "# one, two = this.getData()\n",
    "# chicagoZip = [60601, 60602, 60603, 60604, 60605, 60606, 60607, 60608, 60609, 60610, 60611, 60612, 60613, 60614, 60615, 60616, 60617, 60618, 60619, 60620, 60621, 60622, 60623, 60624, 60625, 60626, 60628, 60629, 60630, 60631, 60632, 60633, 60634, 60636, 60637, 60638, 60639, 60640, 60641, 60642, 60643, 60644, 60645, 60646, 60647, 60649, 60651, 60652, 60653, 60654, 60655, 60656, 60657, 60659, 60660, 60661, 60664, 60666, 60668, 60669, 60670, 60673, 60674, 60675, 60677, 60678, 60680, 60681, 60682, 60684, 60685, 60686, 60687, 60688, 60689, 60690, 60691, 60693, 60694, 60695, 60696, 60697, 60699, 60701, 60706, 60707, 60803, 60804, 60805, 60827]\n",
    "# one[[True if np.isin(i, chicagoZip) else False for i in one['Residence Zip Code']]].to_csv('../data/chicagoParole2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
